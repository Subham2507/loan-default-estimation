{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9615111,"sourceType":"datasetVersion","datasetId":5867614}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, roc_curve, auc\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\n# !pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE","metadata":{"id":"olQE93Yeqrdv","outputId":"351b47ef-7793-462c-8b5e-67d5145e4595","execution":{"iopub.status.busy":"2024-10-21T17:44:29.240196Z","iopub.execute_input":"2024-10-21T17:44:29.240591Z","iopub.status.idle":"2024-10-21T17:44:30.733391Z","shell.execute_reply.started":"2024-10-21T17:44:29.240550Z","shell.execute_reply":"2024-10-21T17:44:30.732599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:30.735050Z","iopub.execute_input":"2024-10-21T17:44:30.735472Z","iopub.status.idle":"2024-10-21T17:44:30.739711Z","shell.execute_reply.started":"2024-10-21T17:44:30.735439Z","shell.execute_reply":"2024-10-21T17:44:30.738790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and prepreprocess the data","metadata":{"id":"By7i4om8qrd2"}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/dataset/accepted_2007_to_2018Q4.csv')","metadata":{"id":"OcNKB-liqrd5","outputId":"f45e233f-d3da-497c-b3f4-0dbbb0ba3fe9","execution":{"iopub.status.busy":"2024-10-21T17:44:30.740872Z","iopub.execute_input":"2024-10-21T17:44:30.741168Z","iopub.status.idle":"2024-10-21T17:44:42.976486Z","shell.execute_reply.started":"2024-10-21T17:44:30.741136Z","shell.execute_reply":"2024-10-21T17:44:42.972697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = data['installment']\na","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.977324Z","iopub.status.idle":"2024-10-21T17:44:42.977693Z","shell.execute_reply.started":"2024-10-21T17:44:42.977515Z","shell.execute_reply":"2024-10-21T17:44:42.977535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Missing Values in Each Column:\")\nprint(data.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.979022Z","iopub.status.idle":"2024-10-21T17:44:42.979457Z","shell.execute_reply.started":"2024-10-21T17:44:42.979253Z","shell.execute_reply":"2024-10-21T17:44:42.979287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Identifying unique values in columns to get more knowledge about the data, which will facilitate in performing one-hot encoding efficiently","metadata":{}},{"cell_type":"code","source":"print(data['home_ownership'].unique(),\ndata['verification_status'].unique(),\ndata['application_type'].unique(),\ndata['purpose'].unique(),\ndata['term'].unique(),\ndata['grade'].unique())","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.980558Z","iopub.status.idle":"2024-10-21T17:44:42.980938Z","shell.execute_reply.started":"2024-10-21T17:44:42.980744Z","shell.execute_reply":"2024-10-21T17:44:42.980762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Perform mean of mode interpolation for the Null values (as mentioned in the paper)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T08:35:33.113536Z","iopub.execute_input":"2024-10-20T08:35:33.114178Z","iopub.status.idle":"2024-10-20T08:35:33.118143Z","shell.execute_reply.started":"2024-10-20T08:35:33.114140Z","shell.execute_reply":"2024-10-20T08:35:33.117267Z"}}},{"cell_type":"code","source":"column_modes = data.mode(dropna=True).iloc[0]    \ndata = data.fillna(column_modes)\ndata = data.fillna(data.mode().iloc[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.981820Z","iopub.status.idle":"2024-10-21T17:44:42.982160Z","shell.execute_reply.started":"2024-10-21T17:44:42.981986Z","shell.execute_reply":"2024-10-21T17:44:42.982004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum().agg","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.983441Z","iopub.status.idle":"2024-10-21T17:44:42.983802Z","shell.execute_reply.started":"2024-10-21T17:44:42.983630Z","shell.execute_reply":"2024-10-21T17:44:42.983649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Identify categorical columns and performing one-hot encoding to perform the resursive feature elimination","metadata":{}},{"cell_type":"code","source":"#X = data.drop('loan_status', axis=1)\n#y = data['loan_status']\n\n# # Identify categorical columns (change according to your dataset)\n#categorical_columns = X.select_dtypes(include=['object']).columns","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.985136Z","iopub.status.idle":"2024-10-21T17:44:42.985508Z","shell.execute_reply.started":"2024-10-21T17:44:42.985323Z","shell.execute_reply":"2024-10-21T17:44:42.985343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(categorical_columns)\n#X = pd.get_dummies(X, columns=categorical_columns)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.986619Z","iopub.status.idle":"2024-10-21T17:44:42.986982Z","shell.execute_reply.started":"2024-10-21T17:44:42.986800Z","shell.execute_reply":"2024-10-21T17:44:42.986818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RFE (Recursive feature elimination)\n- perform appropriate scaling and Recursive feature elimination to extract top 30 features.\n- Here, randomforestclassifier is used as ML model for performing the recursive feature elimination","metadata":{}},{"cell_type":"code","source":"# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X)\n\n# # Initialize the base classifier\n# classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# # Initialize RFE with the classifier, to reduce features to 30\n# selector = RFE(estimator=classifier, n_features_to_select=30, step=1)\n\n# # Fit RFE on the scaled and encoded dataset\n# selector.fit(X_scaled, y)\n\n# # Get the support array (mask) of selected features\n# selected_features = X.columns[selector.support_]\n\n# # Filter the dataset to keep only selected features\n# X_selected = X[selected_features]\n\n# # Output the selected features\n# print(\"Selected features:\", selected_features.tolist())","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.987848Z","iopub.status.idle":"2024-10-21T17:44:42.988167Z","shell.execute_reply.started":"2024-10-21T17:44:42.988003Z","shell.execute_reply":"2024-10-21T17:44:42.988020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)  # Show all columns\npd.set_option('display.expand_frame_repr', False)\nprint(list(data.dtypes))\nprint(list(data.columns))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.989603Z","iopub.status.idle":"2024-10-21T17:44:42.989950Z","shell.execute_reply.started":"2024-10-21T17:44:42.989776Z","shell.execute_reply":"2024-10-21T17:44:42.989795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Note: \n- On further analysing the data, there were one more categorical value in the loan_status column which needs to handle correclty, which was not mentioned in the paper. The given value corelates with fully paid so I added that value with 0 labelling.","metadata":{}},{"cell_type":"code","source":"# count_charged_off = data[data['loan_status'] == 'Does not meet the credit policy. Status:Fully Paid'].shape[0]\n# print(count_charged_off)\n# data['loan_status'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.993186Z","iopub.status.idle":"2024-10-21T17:44:42.993581Z","shell.execute_reply.started":"2024-10-21T17:44:42.993395Z","shell.execute_reply":"2024-10-21T17:44:42.993415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Conversion: Installment -> Installment_feat\nconversion of installment to installment feat, which is fraction of monthly installment with respect to the monthly income. Here we need to handle extreme division in the data values and also calculating the monthly income based other column.","metadata":{}},{"cell_type":"code","source":"condition = data['annual_inc'] == 0.0\ndata = data.drop(data[condition].index)\ndata['monthly_income'] = data['annual_inc'] / 12\nprint(data['annual_inc'].max(), data['monthly_income'].min(), data['monthly_income'].max())","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.995002Z","iopub.status.idle":"2024-10-21T17:44:42.995364Z","shell.execute_reply.started":"2024-10-21T17:44:42.995165Z","shell.execute_reply":"2024-10-21T17:44:42.995182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert specified categorical columns into binary nominal columns\n- Due to limitation of resources and cpu. I need to manually form the one hot encoding on the few columns named home_ownership, verification_status, application_Type, loan_Status, purpose, and term.\n- First I have extracted all the unique values and then map each value to a new binary column.\n- Grade column is mapped to numerical values corresponds to its alphabetic values.\n- At last 30 columns have been extracted, according to the dataset.","metadata":{}},{"cell_type":"code","source":"# Calculate installment_feat\ndata['monthly_income'] = data['annual_inc'] / 12\ndata['installment_feat'] = (data['installment'] / data['monthly_income']) * 100\n\n# Convert specified categorical columns into binary nominal columns\n# Home Ownership\ndata['home_ownership_MORTGAGE'] = (data['home_ownership'] == 'MORTGAGE').astype(int)\ndata['home_ownership_RENT'] = (data['home_ownership'] == 'RENT').astype(int)\ndata['home_ownership_OWN'] = (data['home_ownership'] == 'OWN').astype(int)\ndata['home_ownership_ANY'] = (data['home_ownership'] == 'ANY').astype(int)\n\n# Verification Status\ndata['verification_status_Not_Verified'] = (data['verification_status'] == 'Not Verified').astype(int)\ndata['verification_status_Source_Verified'] = (data['verification_status'] == 'Source Verified').astype(int)\ndata['verification_status_Verified'] = (data['verification_status'] == 'Verified').astype(int)\n\n# Application Type\ndata['application_type_Individual'] = (data['application_type'] == 'Individual').astype(int)\ndata['application_type_Joint_App'] = (data['application_type'] == 'Joint App').astype(int)\n\n# Purpose\ndata['purpose_major_purchase'] = (data['purpose'] == 'major_purchase').astype(int)\ndata['purpose_renewable_energy'] = (data['purpose'] == 'renewable_energy').astype(int)\ndata['purpose_small_business'] = (data['purpose'] == 'small_business').astype(int)\ndata['purpose_vacation'] = (data['purpose'] == 'vacation').astype(int)\n\n# Term\ndata['term'] = data['term'].str.strip().str.lower()\ndata['term_36_months'] = (data['term'] == '36 months').astype(int)\ndata['term_60_months'] = (data['term'] == '60 months').astype(int)\n\nstatus_mapping = {\n    'Current': 0, 'Fully Paid': 0, 'Issued': 0, 'Does not meet the credit policy. Status:Fully Paid':0,\n    'Default': 1, 'Charged Off': 1, 'In Grace Period': 1, 'Does not meet the credit policy. Status:Charged Off': 1,\n    'Late (16-30 days)': 1, 'Late (31-120 days)': 1\n}\n\n# Apply the mapping to the loan_status column\ndata['loan_status_encoded'] = data['loan_status'].map(status_mapping)\n\n\ngrade_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n\ndata['grade'] = data['grade'].map(grade_mapping)\n\n# Select only the required columns (existing + newly created)\ncolumns_to_keep = [\n    'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'installment',\n    'grade', 'open_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n    'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'open_act_il',\n    'delinq_amnt', 'num_op_rev_tl', 'home_ownership_MORTGAGE',\n    'home_ownership_RENT', 'home_ownership_OWN', 'home_ownership_ANY',\n    'verification_status_Not_Verified', 'verification_status_Source_Verified',\n    'verification_status_Verified', 'application_type_Individual',\n    'application_type_Joint_App', 'purpose_major_purchase', 'purpose_renewable_energy',\n    'purpose_small_business', 'purpose_vacation', 'term_36_months', 'term_60_months',\n    'installment_feat','loan_status_encoded'\n]\n\ndata_final = data[columns_to_keep]","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.997339Z","iopub.status.idle":"2024-10-21T17:44:42.997728Z","shell.execute_reply.started":"2024-10-21T17:44:42.997532Z","shell.execute_reply":"2024-10-21T17:44:42.997551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- working with installment and installment feat.","metadata":{}},{"cell_type":"code","source":"data_final.drop(['installment'],axis =1, inplace = True)\nlen(data_final.columns)\nprint(data_final.dtypes)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:42.999213Z","iopub.status.idle":"2024-10-21T17:44:42.999611Z","shell.execute_reply.started":"2024-10-21T17:44:42.999430Z","shell.execute_reply":"2024-10-21T17:44:42.999449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_final.to_csv('30_columns.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.001198Z","iopub.status.idle":"2024-10-21T17:44:43.001593Z","shell.execute_reply.started":"2024-10-21T17:44:43.001391Z","shell.execute_reply":"2024-10-21T17:44:43.001410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/30_columns.csv\")\nprint(data_final.shape)\ndf = data_final","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.002902Z","iopub.status.idle":"2024-10-21T17:44:43.003244Z","shell.execute_reply.started":"2024-10-21T17:44:43.003071Z","shell.execute_reply":"2024-10-21T17:44:43.003089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### SMOTE \n- As mentioned above, the target variable ‘loans status’ has a large difference in the number of normal and\r\ndefault categories, which will cause trouble to model learning. The method of oversampling is used to handle\r\nsample imbalance problem, we adopt SMOTE (Synthetic Minority Oversampling Technique) method in this\r\npaper\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_class_distribution_pie(y):\n    normal_percentage = (sum(y == 0) / len(y)) * 100\n    default_percentage = (sum(y == 1) / len(y)) * 100\n\n    labels = 'Normal', 'Default'\n    sizes = [normal_percentage, default_percentage]\n    colors = ['blue', 'red']\n    explode = (0.1, 0)  \n\n    # Plot\n    plt.figure(figsize=(6, 4))\n    plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n            autopct='%1.2f%%', shadow=True, startangle=140)\n    plt.axis('equal')  \n    plt.title('Percentage of Each Loan Status Before SMOTE')\n    plt.show()\n\nplot_class_distribution_pie(data_final['loan_status_encoded'])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.004747Z","iopub.status.idle":"2024-10-21T17:44:43.005122Z","shell.execute_reply.started":"2024-10-21T17:44:43.004919Z","shell.execute_reply":"2024-10-21T17:44:43.004937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Check for infinite values\n# print(\"Infinite values in X_train:\", np.isinf(X_train).sum().sum())\n# print(\"Infinite values in X_test:\", np.isinf(X_test).sum().sum())\n# # Optionally, drop rows with NaNs if imputation isn't suitable\n# # Replace infinities with NaN\n# X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n# X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# X_train.dropna(inplace=True)\n# y_train = y_train[X_train.index]  # Make sure to align y_train with X_train after dropping\n\n# X_test.dropna(inplace=True)\n# y_test = y_test[X_test.index]  # Align y_test as well\n\n# # Check again for any infinities or NaNs\n# print(\"NaNs in X_train after processing:\", X_train.isnull().sum().sum())\n# print(\"Infinities in X_train after processing:\", np.isinf(X_train).sum().sum())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.006434Z","iopub.status.idle":"2024-10-21T17:44:43.006930Z","shell.execute_reply.started":"2024-10-21T17:44:43.006673Z","shell.execute_reply":"2024-10-21T17:44:43.006699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\n# Separate features and target\nX = df.drop(['loan_status_encoded'], axis=1)  \ny = df['loan_status_encoded']\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize SMOTE and resample the data\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n","metadata":{"id":"0-KUNlwTqyfb","execution":{"iopub.status.busy":"2024-10-21T17:44:43.008357Z","iopub.status.idle":"2024-10-21T17:44:43.008723Z","shell.execute_reply.started":"2024-10-21T17:44:43.008542Z","shell.execute_reply":"2024-10-21T17:44:43.008561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_class_distribution_pie(y_train_resampled)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.010154Z","iopub.status.idle":"2024-10-21T17:44:43.010513Z","shell.execute_reply.started":"2024-10-21T17:44:43.010337Z","shell.execute_reply":"2024-10-21T17:44:43.010356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.013055Z","iopub.status.idle":"2024-10-21T17:44:43.013437Z","shell.execute_reply.started":"2024-10-21T17:44:43.013235Z","shell.execute_reply":"2024-10-21T17:44:43.013253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pearson correlation graph\n- Due to resource constraints, performing RFE for all columns in not feasible and appropriate code has been added previously. here we had manually implmented the correlation matrix for the30 columns which are mentioned in the research paper. ","metadata":{}},{"cell_type":"code","source":"selected_columns = [\n    'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n    'grade', 'open_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n    'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'open_act_il',\n    'delinq_amnt', 'num_op_rev_tl', 'home_ownership_MORTGAGE',\n    'home_ownership_RENT', 'home_ownership_OWN', 'home_ownership_ANY',\n    'verification_status_Not_Verified', 'verification_status_Source_Verified',\n    'verification_status_Verified', 'application_type_Individual',\n    'application_type_Joint_App', 'purpose_major_purchase', 'purpose_renewable_energy',\n    'purpose_small_business', 'purpose_vacation', 'term_36_months', 'term_60_months',\n    'installment_feat'\n]\nlen(selected_columns)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.014784Z","iopub.status.idle":"2024-10-21T17:44:43.015125Z","shell.execute_reply.started":"2024-10-21T17:44:43.014955Z","shell.execute_reply":"2024-10-21T17:44:43.014973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nselected_data_30 = df[selected_columns]\nselected_data_30_scaled = scaler.fit_transform(selected_data_30)\nselected_data_30_scaled = pd.DataFrame(selected_data_30_scaled, columns=selected_columns)\n\ncorrelation_matrix_30 = selected_data_30_scaled.corr()\n\nplt.figure(figsize=(15, 12))\nsns.heatmap(correlation_matrix_30, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Pearson Correlation of 30 Selected Features\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.016374Z","iopub.status.idle":"2024-10-21T17:44:43.016742Z","shell.execute_reply.started":"2024-10-21T17:44:43.016559Z","shell.execute_reply":"2024-10-21T17:44:43.016578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Pearson correlation graph to Identify and remove highly correlated features.\n- We plotted the Pearson correlation graph of the 30 features. \nOn the basis of the first dimension reduction, redundant features are selected and eliminated by Pearso \r\ncorrelation graph, the dimension of features is reduced from 30 to 15","metadata":{}},{"cell_type":"code","source":"columns_to_remove = set()\ncorrelation_threshold = 0.8\n\nfor i in range(len(correlation_matrix_30.columns)):\n    for j in range(i):\n        if abs(correlation_matrix_30.iloc[i, j]) > correlation_threshold:\n            # Identify the feature to remove based on a criterion, such as lower variance\n            if selected_data_30[correlation_matrix_30.columns[i]].var() > selected_data_30[correlation_matrix_30.columns[j]].var():\n                columns_to_remove.add(correlation_matrix_30.columns[j])\n            else:\n                columns_to_remove.add(correlation_matrix_30.columns[i])\n\nremoved_due_to = {col: correlation_matrix_30[col][correlation_matrix_30[col] > correlation_threshold].index.tolist() \n                  for col in columns_to_remove}\n\n# Retain the first 15 features after removing highly correlated ones\nreduced_columns = [col for col in selected_columns if col not in columns_to_remove][:15]\n\n# Get the final dataset with reduced features\nfinal_data_15 = selected_data_30[reduced_columns]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.017943Z","iopub.status.idle":"2024-10-21T17:44:43.018262Z","shell.execute_reply.started":"2024-10-21T17:44:43.018099Z","shell.execute_reply":"2024-10-21T17:44:43.018116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate and plot the Pearson correlation for the 15 features\ncorrelation_matrix_15 = final_data_15.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix_15, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Pearson Correlation of 15 Reduced Features\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.019319Z","iopub.status.idle":"2024-10-21T17:44:43.019644Z","shell.execute_reply.started":"2024-10-21T17:44:43.019474Z","shell.execute_reply":"2024-10-21T17:44:43.019496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Manual feature extraction\n- As mentioned above, RFE was not feasible. so the top 15 columns will not match with the paper. here we have manually extracted 15 feature as mentioned in the paper. Also ensure to perform all the preprocessing mentioned in the paper.\n","metadata":{}},{"cell_type":"code","source":"#15 features\nselected_features = [\n    'loan_amnt', 'installment', 'grade', 'open_acc', 'total_pymnt', 'total_rec_int',\n    'home_ownership_MORTGAGE', 'home_ownership_ANY', \n    'verification_status_Not_Verified', 'application_type_Individual',\n    'purpose_major_purchase', 'purpose_renewable_energy', 'purpose_small_business', 'purpose_vacation',\n    'term_36_months'\n]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.021101Z","iopub.status.idle":"2024-10-21T17:44:43.021582Z","shell.execute_reply.started":"2024-10-21T17:44:43.021331Z","shell.execute_reply":"2024-10-21T17:44:43.021356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate and plot the Pearson correlation for the 15 features\ncorrelation_matrix_15 = data[selected_features].corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix_15, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Pearson Correlation of 15 Reduced Features\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.023004Z","iopub.status.idle":"2024-10-21T17:44:43.023493Z","shell.execute_reply.started":"2024-10-21T17:44:43.023221Z","shell.execute_reply":"2024-10-21T17:44:43.023246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The ranking of the importance of features\n- As mentioned in the paper, plotting the ranking of importance of features. we adopt the Random Forest algorithm to rank the importance of features and reduce the learning \ndifficulty to achieve the purpose of optimizing the model calculation","metadata":{}},{"cell_type":"code","source":"selected_features.remove('installment')\nselected_features","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.025193Z","iopub.status.idle":"2024-10-21T17:44:43.025582Z","shell.execute_reply.started":"2024-10-21T17:44:43.025399Z","shell.execute_reply":"2024-10-21T17:44:43.025418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data_final[selected_features]\ny = data_final['loan_status_encoded']\nsmote = SMOTE(random_state=42)\nX, y = smote.fit_resample(X, y)\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize SMOTE and apply it on the training data\n\n# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Scale the data - important to scale after resampling to avoid data leakage\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.026515Z","iopub.status.idle":"2024-10-21T17:44:43.026863Z","shell.execute_reply.started":"2024-10-21T17:44:43.026687Z","shell.execute_reply":"2024-10-21T17:44:43.026705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Load your dataset\ndata = pd.read_csv('/kaggle/working/30_columns.csv')\ndata = data[selected_columns]\n# Assuming 'loan_status_encoded' is your target and other preprocessing has been done\n# X = data.drop(['loan_status_encoded'], axis=1)\n# y = data['loan_status_encoded']\n# X = df['selected_features']\n\n# Optionally scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n# Initialize the Random Forest Classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict the test set results\ny_pred = rf.predict(X_test)\ny_pred_proba = rf.predict_proba(X_test)[:, 1]  # Probabilities for ROC-AUC\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nroc_auc = roc_auc_score(y_test, y_pred_proba)\n\nprint(f'Accuracy of Random Forest Classifier: {accuracy:.2f}')\nprint(f'ROC-AUC Score: {roc_auc:.2f}')\n\n# Feature Importance Visualization\nimportances = rf.feature_importances_\nindices = pd.DataFrame(importances, index=X.columns).sort_values(0, ascending=False)\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nplt.title('Feature Importances by Random Forest')\nplt.barh(indices.index, indices[0], color='b', align='center')\nplt.xlabel('Relative Importance')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.029042Z","iopub.status.idle":"2024-10-21T17:44:43.029480Z","shell.execute_reply.started":"2024-10-21T17:44:43.029261Z","shell.execute_reply":"2024-10-21T17:44:43.029303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_metrics = {}","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.030939Z","iopub.status.idle":"2024-10-21T17:44:43.031325Z","shell.execute_reply.started":"2024-10-21T17:44:43.031129Z","shell.execute_reply":"2024-10-21T17:44:43.031147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Initialize the Random Forest classifier\nrf_classifier = RandomForestClassifier(random_state=42)\n\n# Train the classifier\nrf_classifier.fit(X_train_scaled, y_train)\n\n# Predict on the test data\nrf_predictions = rf_classifier.predict(X_test_scaled)\nrf_proba = rf_classifier.predict_proba(X_test_scaled)[:, 1]\n\n# Calculate the evaluation metrics\nrf_accuracy = accuracy_score(y_test, rf_predictions)\nrf_auc = roc_auc_score(y_test, rf_proba)\nrf_f1 = f1_score(y_test, rf_predictions)\nrf_recall = recall_score(y_test, rf_predictions)\n\nprint(f\"Random Forest Accuracy: {rf_accuracy}\")\nprint(f\"Random Forest AUC: {rf_auc}\")\nprint(f\"Random Forest F1-Score: {rf_f1}\")\nprint(f\"Random Forest Recall: {rf_recall}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.032608Z","iopub.status.idle":"2024-10-21T17:44:43.032973Z","shell.execute_reply.started":"2024-10-21T17:44:43.032802Z","shell.execute_reply":"2024-10-21T17:44:43.032820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# Initialize the Decision Tree classifier\ndt_classifier = DecisionTreeClassifier(random_state=42)\n\n# Train the classifier\ndt_classifier.fit(X_train_scaled, y_train)\n\n# Predict on the test data\ndt_predictions = dt_classifier.predict(X_test_scaled)\ndt_proba = dt_classifier.predict_proba(X_test_scaled)[:, 1]\n\n# Calculate the evaluation metrics\ndt_accuracy = accuracy_score(y_test, dt_predictions)\ndt_auc = roc_auc_score(y_test, dt_proba)\ndt_f1 = f1_score(y_test, dt_predictions)\ndt_recall = recall_score(y_test, dt_predictions)\n\nprint(f\"Decision Tree Accuracy: {dt_accuracy}\")\nprint(f\"Decision Tree AUC: {dt_auc}\")\nprint(f\"Decision Tree F1-Score: {dt_f1}\")\nprint(f\"Decision Tree Recall: {dt_recall}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.034400Z","iopub.status.idle":"2024-10-21T17:44:43.034783Z","shell.execute_reply.started":"2024-10-21T17:44:43.034582Z","shell.execute_reply":"2024-10-21T17:44:43.034601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score\n\n# Initialize the SVM classifier\nsvm_classifier = SVC(kernel='linear', probability=True, random_state=42)\n\n# Train the classifier\nsvm_classifier.fit(X_train_scaled, y_train)\n\n# Predict on the test data\nsvm_predictions = svm_classifier.predict(X_test_scaled)\nsvm_proba = svm_classifier.predict_proba(X_test_scaled)[:, 1]\n\n# Calculate the evaluation metrics\nsvm_accuracy = accuracy_score(y_test, svm_predictions)\nsvm_auc = roc_auc_score(y_test, svm_proba)\nsvm_f1 = f1_score(y_test, svm_predictions)\nsvm_recall = recall_score(y_test, svm_predictions)\n\nprint(f\"SVM Accuracy: {svm_accuracy}\")\nprint(f\"SVM AUC: {svm_auc}\")\nprint(f\"SVM F1-Score: {svm_f1}\")\nprint(f\"SVM Recall: {svm_recall}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.036079Z","iopub.status.idle":"2024-10-21T17:44:43.036584Z","shell.execute_reply.started":"2024-10-21T17:44:43.036329Z","shell.execute_reply":"2024-10-21T17:44:43.036373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Initialize the Logistic Regression classifier\nlogreg_classifier = LogisticRegression(random_state=42)\n\n# Train the classifier\nlogreg_classifier.fit(X_train_scaled, y_train)\n\n# Predict on the test data\nlogreg_predictions = logreg_classifier.predict(X_test_scaled)\nlogreg_proba = logreg_classifier.predict_proba(X_test_scaled)[:, 1]\n\n# Calculate the evaluation metrics\nlogreg_accuracy = accuracy_score(y_test, logreg_predictions)\nlogreg_auc = roc_auc_score(y_test, logreg_proba)\nlogreg_f1 = f1_score(y_test, logreg_predictions)\nlogreg_recall = recall_score(y_test, logreg_predictions)\n\nprint(f\"Logistic Regression Accuracy: {logreg_accuracy}\")\nprint(f\"Logistic Regression AUC: {logreg_auc}\")\nprint(f\"Logistic Regression F1-Score: {logreg_f1}\")\nprint(f\"Logistic Regression Recall: {logreg_recall}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.037767Z","iopub.status.idle":"2024-10-21T17:44:43.038106Z","shell.execute_reply.started":"2024-10-21T17:44:43.037932Z","shell.execute_reply":"2024-10-21T17:44:43.037950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Create DataFrame from the metrics dictionary\nresults_df = pd.DataFrame(global_metrics).T\n\n# Display the DataFrame\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T17:44:43.039417Z","iopub.status.idle":"2024-10-21T17:44:43.039765Z","shell.execute_reply.started":"2024-10-21T17:44:43.039595Z","shell.execute_reply":"2024-10-21T17:44:43.039614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}